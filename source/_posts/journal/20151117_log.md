title: Log_2015-11-17 -- Standard ArrayWritable, Application of Combiner
date: 2015-11-17
category: Journal
---
### Standard ArrayWritable
A Writable for arrays containing instances of a class. The elements of this writable must all be instances of the same class. If this writable will be the input for a Reducer, you will need to create a subclass that sets the value to be of the proper type. For example:
```java
    public class IntArrayWritable extends ArrayWritable {
      public IntArrayWritable() {
        super(IntWritable.class);
      }
    }
```
```java
import java.io.IOException;
import java.util.Iterator;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.ArrayWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;

public class WordCount {

    public static class TextArrayWritable extends ArrayWritable {
        public TextArrayWritable() {
            super(Text.class);
        }

        public TextArrayWritable(String[] strings) {
            super(Text.class);
            Text[] texts = new Text[strings.length];
            for (int i = 0; i < strings.length; i++) {
                texts[i] = new Text(strings[i]);
            }
            set(texts);
        }
    }

    public static class MapClass extends MapReduceBase implements
            Mapper<LongWritable, Text, Text, ArrayWritable> {
        public void map(LongWritable key, Text value,
                        OutputCollector<Text, ArrayWritable> output, Reporter reporter)
                throws IOException {

            String[] arr_str = new String[] {
                    "a", "b", "c" };
            for (int i = 0; i < 3; i++)
                output.collect(new Text("my_key"), new TextArrayWritable(
                        arr_str));
        }
    }

    public static class Reduce extends MapReduceBase implements
            Reducer<Text, TextArrayWritable, Text, TextArrayWritable> {

        public void reduce(Text key, Iterator<TextArrayWritable> values,
                           OutputCollector<Text, TextArrayWritable> output,
                           Reporter reporter) throws IOException {

            TextArrayWritable tmp;

            while (values.hasNext()) {
                tmp = values.next();
                output.collect(key, tmp);
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

        JobConf job = new JobConf(conf, WordCount.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(TextArrayWritable.class);
        job.setOutputFormat(TextOutputFormat.class);
        job.setInputFormat(TextInputFormat.class);

        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setJobName("TempClass");

        JobClient.runJob(job);
    }
}
```

### Application of Combiner
**Import**: The output Key/Value format of Combiner has to be same as Mapper's output format, and the input Key/Value format of Combiner has to be same as Reducer's input Key/Value format.


### References
**map/reduce 下ArrayWritable 的使用**
* 1.在map/reduce的开发中经常使用数组类型, 例如DoubleWritable ,当需要使用一组DoubleWritable的时候,
```java
DoubleWritable[] doublewritable =new DoubleWritable[n]
```
即可.

* 2.但是DoubleWritable[] 这种形式的数组并不能作为value在map和reduce中传递. 实际上,map/reduce 的序列类型为ArrayWritable.
当我要使用DoubleWritable类型的数组作为value进行传递时,需要重写一个类继承于ArrayWritable
```java
public class NewArrayWritable extends ArrayWritable{
	public NewArrayWritable() {
	     super(DoubleWritable.class);
	   }
}
```
* 3.当要使用时:
```java
NewArrayWritable ntemp=new NewArrayWritable();
//如果我现在有一个DoubleWritable[]  ctemp, 我可以这样赋值:
ntemp.set(ctemp);
```
* 4.当修改map的输出key类型时,需要对如下语句进行修改
```java
job.setMapOutputValueClass(IntWritable.class);
改为
job.setMapOutputValueClass(NewArrayWritable.class);
```

* 5.此时程序可能出现wrong value class: class org.apache.hadoop.io.IntWritable is not class org.apache.hadoop.examples.NewArrayWritable 异常
解决方法: 注释掉main函数中
```java
job.setCombinerClass(IntSumReducer.class);
```

### Key/Value Separator
```java
    Configuration conf = new Configuration();
    conf.set("mapreduce.output.textoutputformat.separator", "-----")
```
